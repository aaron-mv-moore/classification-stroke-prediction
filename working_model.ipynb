{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c018904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.12.1.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# basic libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "#visulaization modules\n",
    "import missingno as msno\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "# !pip install pywaffle\n",
    "# from pywaffle import Waffle\n",
    "\n",
    "%matplotlib inline\n",
    "init_notebook_mode(connected= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d731110",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2944c8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Common model helpers\n",
    "from sklearn.preprocessing import (StandardScaler,\n",
    "                                   LabelEncoder,\n",
    "                                   OneHotEncoder)\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, \n",
    "                             auc, \n",
    "                             precision_score,\n",
    "                             recall_score,\n",
    "                             f1_score, \n",
    "                             roc_auc_score,\n",
    "                             confusion_matrix)\n",
    "from sklearn.model_selection import (GridSearchCV,\n",
    "                                     StratifiedKFold,\n",
    "                                     cross_val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b41cb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "# from umap import UMAP\n",
    "import pylab as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5901445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imbalance dataset handling\n",
    "\n",
    "from imblearn.datasets import make_imbalance\n",
    "from imblearn.under_sampling import (RandomUnderSampler, \n",
    "                                     ClusterCentroids,\n",
    "                                     TomekLinks,\n",
    "                                     NeighbourhoodCleaningRule,\n",
    "                                     EditedNearestNeighbours,\n",
    "                                     NearMiss)\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import (SMOTE,\n",
    "                                    ADASYN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcda98ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model algorithams\n",
    "from sklearn.ensemble import (RandomForestClassifier, \n",
    "                              AdaBoostClassifier, \n",
    "                              GradientBoostingClassifier)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "# from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e666b453",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32ac0f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wrangle import wrangle_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "147d12d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = wrangle_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89349d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'stroke'\n",
    "features = ['hypertension', 'heart_disease', 'ever_married', 'work_type', 'age_bins', 'glucose_bins', 'bmi_bins']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "628bd253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    '''\n",
    "    Actions:\n",
    "    Returns: X_train, y_train, X_validate, y_validate, X_test, y_test\n",
    "    Modules:\n",
    "        1. import pandas as pd\n",
    "        2. from wrangle import wrangle_data\n",
    "    '''\n",
    "    # get data\n",
    "    train, validate, test = wrangle_data()\n",
    "    \n",
    "    # set target\n",
    "    target = ['stroke']\n",
    "    \n",
    "    # set features of interest\n",
    "    features = ['hypertension', 'heart_disease', 'ever_married', 'work_type', 'age_bins', 'glucose_bins', 'bmi_bins', 'smoking_status']\n",
    "    \n",
    "    # create train X, y\n",
    "    X_train = train[features]\n",
    "    y_train = train[target]\n",
    "    \n",
    "    # create validate X, y\n",
    "    X_validate = validate[features]\n",
    "    y_validate = validate[target]\n",
    "    \n",
    "    # create test X, y\n",
    "    X_test = test[features]\n",
    "    y_test = test[target]\n",
    "    \n",
    "    return X_train, y_train, X_validate, y_validate, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "091fc0d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['smoking_status'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X_train, y_train, X_validate, y_validate, X_test, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mpreprocess\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhypertension\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheart_disease\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mever_married\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwork_type\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage_bins\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglucose_bins\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbmi_bins\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msmoking_status\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# create train X, y\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     20\u001b[0m y_train \u001b[38;5;241m=\u001b[39m train[target]\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# create validate X, y\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py:3511\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3509\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3510\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3511\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3513\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py:5796\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5793\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5794\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 5796\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5798\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   5799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   5800\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py:5859\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5856\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5858\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 5859\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['smoking_status'] not in index\""
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_validate, y_validate, X_test, y_test = preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b946f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eab3adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train copy\n",
    "X = train[features]\n",
    "y_train = train['stroke']\n",
    "\n",
    "ordinal = ['age_bins',  'glucose_bins', 'bmi_bins', 'hypertension', 'heart_disease']\n",
    "\n",
    "for col in ordinal:\n",
    "    X[col] = le.fit_transform(X[col])\n",
    "\n",
    "nominal =  ['ever_married', 'work_type', 'smo']\n",
    "\n",
    "## norminal data one hot encoding for categorical features\n",
    "temp = X.drop(columns = nominal)\n",
    "dummies = pd.get_dummies(X[nominal])\n",
    "X = pd.concat([temp,dummies], axis = 1)\n",
    "\n",
    "X_train = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f78254c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train copy\n",
    "X = validate[features]\n",
    "y_validate = validate['stroke']\n",
    "\n",
    "ordinal = ['age_bins',  'glucose_bins', 'bmi_bins', 'hypertension', 'heart_disease']\n",
    "\n",
    "for col in ordinal:\n",
    "    X[col] = le.fit_transform(X[col])\n",
    "\n",
    "nominal =  ['ever_married', 'work_type']\n",
    "\n",
    "## norminal data one hot encoding for categorical features\n",
    "temp = X.drop(columns = nominal)\n",
    "dummies = pd.get_dummies(X[nominal])\n",
    "X = pd.concat([temp,dummies], axis = 1)\n",
    "\n",
    "X_validate = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86adfa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce13687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversamplling\n",
    "smote = SMOTE()\n",
    "X_resample, y_resample = smote.fit_resample(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9385cc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resample.shape, X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49a8995",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4290515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### predictions with resampled data\n",
    "\n",
    "\n",
    "def predictions(x_set,y_set):\n",
    "    t1 = time.time()\n",
    "    print('Classification Process Starts....')\n",
    "    accuracy,precision,recall,f1,auc,conf_mat= [],[],[],[],[],[]\n",
    "        \n",
    "    random_state = 1017\n",
    "    \n",
    "    ##classifiers list \n",
    "    classifiers = []\n",
    "    classifiers.append(SVC(random_state=random_state, probability = True))\n",
    "    classifiers.append(DecisionTreeClassifier(random_state=random_state))\n",
    "    classifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state)))\n",
    "    classifiers.append(RandomForestClassifier(random_state=random_state))\n",
    "    classifiers.append(GradientBoostingClassifier(random_state=random_state))\n",
    "    classifiers.append(KNeighborsClassifier())\n",
    "    classifiers.append(LogisticRegression(random_state = random_state))\n",
    "    classifiers.append(XGBClassifier(random_state = random_state, eval_metric = 'logloss', learning_rate = 0.054))\n",
    "    \n",
    "\n",
    "\n",
    "    for classifier in classifiers:\n",
    "        \n",
    "        t =time.time()\n",
    "        print('fitting on classifier with parameters: {}'.format(classifier))\n",
    "        \n",
    "        #classifier and fitting\n",
    "        clf = classifier\n",
    "        clf.fit(x_set,y_set)\n",
    "        \n",
    "        #predictions\n",
    "        y_preds = clf.predict(X_validate)\n",
    "        y_probs = clf.predict_proba(X_validate)\n",
    "        \n",
    "        # metrics\n",
    "        accuracy.append((round(accuracy_score(y_validate,y_preds),2))*100)\n",
    "        precision.append((round(precision_score(y_validate,y_preds),2))*100)\n",
    "        recall.append((round(recall_score(y_validate,y_preds),2))*100)\n",
    "        f1.append((round(f1_score(y_validate,y_preds),2))*100)\n",
    "        auc.append((round (roc_auc_score(y_validate,y_probs[:,1]), 2))*100)\n",
    "        conf_mat.append(confusion_matrix(y_validate,y_preds))\n",
    "        \n",
    "        elapsed = time.time() - t\n",
    "        print('Done and elapsed time is {}seconds'.format(round(elapsed,3)))\n",
    "        print('\\n')\n",
    "    results_df = pd.DataFrame({\"Accuracy Score\":accuracy,\"Precision Score\":precision,\n",
    "                        \"Recall Score\":recall, \"f1 Score\":f1,\"AUC Score\":auc,\n",
    "                        \"Confusion Matrix\":conf_mat,\n",
    "                        \"Algorithm\":[\"SVC\",\"DecisionTree\",\"AdaBoost\",\n",
    "                                     \"RandomForest\",\"GradientBoosting\",\n",
    "                                     \"KNeighboors\",\"LogisticRegression\",\n",
    "                                     \"XGBoost\"]})\n",
    "    \n",
    "    results_df = (results_df.sort_values(by = 'Algorithm', ascending = False)\n",
    "                  .reset_index(drop =  True))\n",
    "    t2 = time.time() - t1\n",
    "    print('\\nClassification is Completed and results are strored in dataframe.\\ntotal time elapsed is {}seconds'.format(t2))\n",
    "    print('***************************************************************\\n\\n')\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e22774a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e448af",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_results = predictions(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb815e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907fb5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "resamp_results = predictions(X_resample, y_resample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e97e423",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resamp_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10eb7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72d7353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi_visualize(data = [orig_results, resamp_results], vmin=30,vmax = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82767fae",
   "metadata": {},
   "source": [
    "# without heart disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08424a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'stroke'\n",
    "features = ['hypertension', 'ever_married', 'work_type', 'age_bins', 'glucose_bins', 'bmi_bins']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e8a7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train copy\n",
    "X = validate[features]\n",
    "y_validate = validate['stroke']\n",
    "\n",
    "ordinal = ['age_bins',  'glucose_bins', 'bmi_bins', 'hypertension']\n",
    "\n",
    "for col in ordinal:\n",
    "    X[col] = le.fit_transform(X[col])\n",
    "\n",
    "nominal =  ['ever_married', 'work_type']\n",
    "\n",
    "## norminal data one hot encoding for categorical features\n",
    "temp = X.drop(columns = nominal)\n",
    "dummies = pd.get_dummies(X[nominal])\n",
    "X = pd.concat([temp,dummies], axis = 1)\n",
    "\n",
    "X_validate = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3499e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train copy\n",
    "X = train[features]\n",
    "y_train = train['stroke']\n",
    "\n",
    "ordinal = ['age_bins',  'glucose_bins', 'bmi_bins', 'hypertension']\n",
    "\n",
    "for col in ordinal:\n",
    "    X[col] = le.fit_transform(X[col])\n",
    "\n",
    "nominal =  ['ever_married', 'work_type']\n",
    "\n",
    "## norminal data one hot encoding for categorical features\n",
    "temp = X.drop(columns = nominal)\n",
    "dummies = pd.get_dummies(X[nominal])\n",
    "X = pd.concat([temp,dummies], axis = 1)\n",
    "\n",
    "X_train = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6717b1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversamplling\n",
    "smote = SMOTE()\n",
    "X_resample, y_resample = smote.fit_resample(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124f99c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_results = predictions(X_train, y_train)\n",
    "resamp_results = predictions(X_resample, y_resample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfd4945",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dbc929",
   "metadata": {},
   "outputs": [],
   "source": [
    "resamp_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9ad8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### predictions with resampled data\n",
    "\n",
    "\n",
    "def test_predictions(x_set,y_set):\n",
    "    t1 = time.time()\n",
    "    print('Classification Process Starts....')\n",
    "    accuracy,precision,recall,f1,auc,conf_mat= [],[],[],[],[],[]\n",
    "        \n",
    "    random_state = 1017\n",
    "    \n",
    "    ##classifiers list \n",
    "    classifiers = []\n",
    "    classifiers.append(SVC(random_state=random_state, probability = True))\n",
    "    classifiers.append(LogisticRegression(random_state = random_state))\n",
    "\n",
    "\n",
    "    for classifier in classifiers:\n",
    "        \n",
    "        t =time.time()\n",
    "        print('fitting on classifier with parameters: {}'.format(classifier))\n",
    "        \n",
    "        #classifier and fitting\n",
    "        clf = classifier\n",
    "        clf.fit(x_set,y_set)\n",
    "        \n",
    "        #predictions\n",
    "        y_preds = clf.predict(X_test)\n",
    "        y_probs = clf.predict_proba(X_test)\n",
    "        \n",
    "        # metrics\n",
    "        accuracy.append((round(accuracy_score(y_test,y_preds),2))*100)\n",
    "        precision.append((round(precision_score(y_test,y_preds),2))*100)\n",
    "        recall.append((round(recall_score(y_test,y_preds),2))*100)\n",
    "        f1.append((round(f1_score(y_test,y_preds),2))*100)\n",
    "        auc.append((round (roc_auc_score(y_test,y_probs[:,1]), 2))*100)\n",
    "        conf_mat.append(confusion_matrix(y_test,y_preds))\n",
    "        \n",
    "        elapsed = time.time() - t\n",
    "        print('Done and elapsed time is {}seconds'.format(round(elapsed,3)))\n",
    "        print('\\n')\n",
    "    results_df = pd.DataFrame({\"Accuracy Score\":accuracy,\"Precision Score\":precision,\n",
    "                        \"Recall Score\":recall, \"f1 Score\":f1,\"AUC Score\":auc,\n",
    "                        \"Confusion Matrix\":conf_mat,\n",
    "                        \"Algorithm\":[\"SVC\", \"LogisticRegression\"]})\n",
    "    \n",
    "    results_df = (results_df.sort_values(by = 'Algorithm', ascending = False)\n",
    "                  .reset_index(drop =  True))\n",
    "    t2 = time.time() - t1\n",
    "    print('\\nClassification is Completed and results are strored in dataframe.\\ntotal time elapsed is {}seconds'.format(t2))\n",
    "    print('***************************************************************\\n\\n')\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd912218",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'stroke'\n",
    "features = ['hypertension', 'heart_disease', 'ever_married', 'work_type', 'age_bins', 'glucose_bins', 'bmi_bins']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f817c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train copy\n",
    "X = train[features]\n",
    "y_train = train['stroke']\n",
    "\n",
    "ordinal = ['age_bins',  'glucose_bins', 'bmi_bins', 'hypertension', 'heart_disease']\n",
    "\n",
    "for col in ordinal:\n",
    "    X[col] = le.fit_transform(X[col])\n",
    "\n",
    "nominal =  ['ever_married', 'work_type']\n",
    "\n",
    "## nominal data one hot encoding for categorical features\n",
    "temp = X.drop(columns = nominal)\n",
    "dummies = pd.get_dummies(X[nominal])\n",
    "X = pd.concat([temp,dummies], axis = 1)\n",
    "\n",
    "X_train = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ede5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train copy\n",
    "X = test[features]\n",
    "y_test = test['stroke']\n",
    "\n",
    "ordinal = ['age_bins',  'glucose_bins', 'bmi_bins', 'hypertension', 'heart_disease']\n",
    "nominal =  ['ever_married', 'work_type']\n",
    "\n",
    "\n",
    "for col in ordinal:\n",
    "    X[col] = le.fit_transform(X[col])\n",
    "\n",
    "## norminal data one hot encoding for categorical features\n",
    "temp = X.drop(columns = nominal)\n",
    "dummies = pd.get_dummies(X[nominal])\n",
    "X = pd.concat([temp,dummies], axis = 1)\n",
    "\n",
    "X_test = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a759ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversamplling\n",
    "smote = SMOTE()\n",
    "X_resample, y_resample = smote.fit_resample(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e397c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "resamp_test = test_predictions(X_resample, y_resample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fac6578",
   "metadata": {},
   "outputs": [],
   "source": [
    "resamp_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018ea6e4",
   "metadata": {},
   "source": [
    "Takeaways:\n",
    "* SVC, Logistic Regression, Gradient Boost perform the best\n",
    "\n",
    "Actions:\n",
    "* Create a baseline and add to the function created\n",
    "* Create a function specifically for this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b785d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_X = [X_train, X_validate, X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b72b52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bd3192",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal = ['age_bins',  'glucose_bins', 'bmi_bins', 'hypertension', 'heart_disease', 'smoking_status']\n",
    "\n",
    "for X in big_X:\n",
    "    for col in ordinal:\n",
    "        X[col] = le.fit_transform(X[col])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78665f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "converted = []\n",
    "for X in big_X:\n",
    "    temp = X.drop(['ever_married', 'work_type', 'smoking_status'], axis=1)\n",
    "#     temp\n",
    "    dummies = pd.get_dummies(X[nominal], drop_first=True)\n",
    "    \n",
    "    converted.append(pd.concat([temp,dummies], axis = 1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d8a686",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_resample, y_resample, X_validate, y_validate, X_test, y_test = preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33be3562",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_validate, X_test = converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747db465",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_features(X_list):\n",
    "    '''\n",
    "    Arguments: [X_train, X_validate, X_test]\n",
    "    Actions:\n",
    "        1. Encodes variables\n",
    "        2. Creates new datasets with encoded variables  \n",
    "    Returns: [X_train_encoded, X_validate_encoded, X_test_encoded] \n",
    "    Modules:\n",
    "        1. import pandas as pd\n",
    "        2. from sklearn.preprocessing import LabelEncoder\n",
    "    '''\n",
    "    # set ordinal variables\n",
    "    ordinal = ['age_bins',  'glucose_bins', 'bmi_bins', 'hypertension', 'heart_disease']\n",
    "    \n",
    "    # set nominal variables\n",
    "    nominal = ['ever_married', 'work_type', 'smoking_status']\n",
    "    \n",
    "    # initialize encoder\n",
    "    le = LabelEncoder()\n",
    "    \n",
    "    # for each dataset\n",
    "    for X in X_list:\n",
    "        \n",
    "        # for each ordinal variable in each dataset\n",
    "        for col in ordinal:\n",
    "            \n",
    "            # fit and transform each and replace the values in the original\n",
    "            X[col] = le.fit_transform(X[col])\n",
    "    \n",
    "    # initialize list\n",
    "    converted = []\n",
    "    \n",
    "    # for each data set\n",
    "    for X in X_list:\n",
    "        \n",
    "        # create temporrary dataset with pre-encoded variables\n",
    "        temp = X.drop(nominal, axis=1)\n",
    "\n",
    "        # get the dummy variables for each nominal variable\n",
    "        dummies = pd.get_dummies(X[nominal], drop_first=True)\n",
    "\n",
    "        # add new datasets with all encoded variables to the list\n",
    "        converted.append(pd.concat([temp, dummies], axis = 1))\n",
    "       \n",
    "    # exit function and return the list of encoded datasets\n",
    "    return converted\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72118f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode_features([X_train, X_validate, X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9f8dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    '''\n",
    "    Actions:\n",
    "        1. Gets data\n",
    "        2. Creates X, y datasets for train, validate, and test\n",
    "        3. Encodes all X datasets\n",
    "        4. Oversamples using X train and y train\n",
    "    Returns: X_train, y_train, X_resample, y_resample, X_validate, y_validate, X_test, y_test\n",
    "    Modules:\n",
    "        1. import pandas as pd\n",
    "        2. from wrangle import wrangle_data\n",
    "        3. from model import encode_features\n",
    "        4. from imblearn.over_sampling import SMOTE\n",
    "    '''\n",
    "    # get data\n",
    "    train, validate, test = wrangle_data()\n",
    "    \n",
    "    # set target\n",
    "    target = 'stroke'\n",
    "    \n",
    "    # set features of interest\n",
    "    features = ['hypertension', 'heart_disease', 'ever_married', 'work_type', 'smoking_status', 'age_bins', 'glucose_bins', 'bmi_bins']\n",
    "    \n",
    "    # create train X, y\n",
    "    X_train = train[features]\n",
    "    y_train = train[target]\n",
    "    \n",
    "    # create validate X, y\n",
    "    X_validate = validate[features]\n",
    "    y_validate = validate[target]\n",
    "    \n",
    "    # create test X, y\n",
    "    X_test = test[features]\n",
    "    y_test = test[target]\n",
    "    \n",
    "    # encoding variables\n",
    "    X_train, X_validate, X_test = encode_features([X_train, X_validate, X_test])\n",
    "    \n",
    "    # initialize oversampling \n",
    "    smote = SMOTE(random_state=1017)\n",
    "    # fit and resample using X and y train\n",
    "    X_resample, y_resample = smote.fit_resample(X_train, y_train.ravel())\n",
    "    \n",
    "    # exit function and return all preprocessed datasets\n",
    "    return X_train, y_train, X_resample, y_resample, X_validate, y_validate, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8958c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_resample, y_resample, X_validate, y_validate, X_test, y_test = preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfd22dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.Series(y_resample).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20ba3c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### predictions with resampled data\n",
    "\n",
    "def predictions(x_set,y_set, X_validate, y_validate):\n",
    "    '''\n",
    "    Actions: Gets dataframe with evaluation scores for SVC, GradientBoost, and LogisticRegression classifiers\n",
    "    '''\n",
    "    \n",
    "    # initialize lists to hold metrics\n",
    "    accuracy,precision,recall,f1,conf_mat= [],[],[],[],[]\n",
    "    \n",
    "    # set a random state\n",
    "    random_state = 1017\n",
    "    \n",
    "    # set baseline predictions\n",
    "    y_preds = np.zeros(len(X_validate)).astype(int)\n",
    "\n",
    "    # adding metrics for baseline\n",
    "    accuracy.append((round(accuracy_score(y_validate,y_preds),2))*100)\n",
    "    precision.append((round(precision_score(y_validate,y_preds),2))*100)\n",
    "    recall.append((round(recall_score(y_validate,y_preds),2))*100)\n",
    "    f1.append((round(f1_score(y_validate,y_preds),2))*100)\n",
    "    conf_mat.append(confusion_matrix(y_validate,y_preds))\n",
    "\n",
    "    \n",
    "    # intitializing different classifiers\n",
    "    clf1 = SVC(random_state=random_state, probability=True)\n",
    "    clf2 = GradientBoostingClassifier(random_state=random_state)\n",
    "    clf3 = LogisticRegression(random_state = random_state)\n",
    "    clf4 = LogisticRegression(C=.25, random_state = random_state)\n",
    "    clf5 = LogisticRegression(C=.5, random_state = random_state)\n",
    "\n",
    "    # initializing voting classifier with top three classifiers from above\n",
    "    eclf = VotingClassifier(estimators=[\n",
    "        ('svc', clf1),('gbc', clf2), ('lr', clf3), ('lr.5', clf4), ('lr.25', clf5)])\n",
    "    \n",
    "    \n",
    "    # initialize classifier list\n",
    "    classifiers = []\n",
    "    \n",
    "    # adding classification models to be used\n",
    "    classifiers.append(clf1)\n",
    "    classifiers.append(clf2)\n",
    "    classifiers.append(clf3)    \n",
    "    classifiers.append(clf4)\n",
    "    classifiers.append(clf5)\n",
    "    classifiers.append(eclf)\n",
    "    \n",
    "    # for each classification method in the list\n",
    "    for clf in classifiers:\n",
    "        \n",
    "        # fit classifier\n",
    "        clf.fit(x_set,y_set)\n",
    "        \n",
    "        # assign predictions to variable\n",
    "        y_preds = clf.predict(X_validate)\n",
    "        \n",
    "        # appending the metrics to each repsective metric list\n",
    "        accuracy.append((round(accuracy_score(y_validate,y_preds),2))*100)\n",
    "        precision.append((round(precision_score(y_validate,y_preds),2))*100)\n",
    "        recall.append((round(recall_score(y_validate,y_preds),2))*100)\n",
    "        f1.append((round(f1_score(y_validate,y_preds),2))*100)\n",
    "        conf_mat.append(confusion_matrix(y_validate,y_preds))\n",
    "\n",
    "    # creating a dataframe with the metrics from the list and each algorithm name\n",
    "    results_df = pd.DataFrame({\"Recall Score\":recall,\n",
    "                               \"Accuracy Score\":accuracy,\n",
    "                               \"Precision Score\":precision,\n",
    "                               \"f1 Score\":f1,\n",
    "                               \"Confusion Matrix\":conf_mat,\n",
    "                               \"Algorithm\":[\"Baseline\",\n",
    "                                            \"SVC\",\n",
    "                                            \"GradientBoosting\",\n",
    "                                            \"LogisticRegression\",\n",
    "                                            \"LR C=.25\",\n",
    "                                            \"LR C=.5\",\n",
    "                                            \"VotingClassifier\"]})\n",
    "                                     \n",
    "    # sorting algorithm name alphabetically and setting index to the algorithm name \n",
    "    results_df = results_df.sort_values(by = 'Algorithm').set_index('Algorithm')\n",
    "    \n",
    "    # exit function and return df\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8060704d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4c3074",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions_difference = predictions_train(X_resample, y_resample) -  predictions_validate(X_resample, y_resample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19105b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_validate(X_resample, y_resample)['Algorithm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468ec1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_difference['Algorithm'] = predictions_validate(X_resample, y_resample)['Algorithm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2015fe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intial predictions on train\n",
    "predictions_train(X_resample, y_resample).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1512aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate predictions scores\n",
    "predictions_validate(X_resample, y_resample).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e32890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference between train and validate\n",
    "predictions_difference.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baef1089",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c6a4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 1017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521c1697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intitializing differenct classifiers\n",
    "clf1 = SVC(random_state=random_state, probability=True)\n",
    "clf2 = GradientBoostingClassifier(random_state=random_state)\n",
    "clf3 = LogisticRegression(random_state = random_state)\n",
    "\n",
    "eclf1 = VotingClassifier(estimators=[\n",
    "    ('svc', clf1), ('lr', clf3), ('gbc', clf2)], voting='hard')\n",
    "\n",
    "eclf2 = VotingClassifier(estimators=[\n",
    "    ('svc', clf1), ('gbc', clf2), ('lr', clf3)], voting='soft')\n",
    "\n",
    "eclf3 = VotingClassifier(estimators=[\n",
    "    ('svc', clf1), ('gbc', clf2), ('lr', clf3)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25adab5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "votes = [eclf1, eclf2, eclf3]\n",
    "# initialize lists to hold metrics\n",
    "accuracy,precision,recall,f1,conf_mat= [],[],[],[],[]\n",
    "\n",
    "# for each classification method in the list\n",
    "for vote in votes:\n",
    "\n",
    "    vote.fit(X_resample, y_resample)\n",
    "\n",
    "    # assign predictions to variable\n",
    "    y_preds = vote.predict(X_train)\n",
    "\n",
    "    # appending the metrics to each repsective metric list\n",
    "    accuracy.append((round(accuracy_score(y_train,y_preds),2))*100)\n",
    "    precision.append((round(precision_score(y_train,y_preds),2))*100)\n",
    "    recall.append((round(recall_score(y_train,y_preds),2))*100)\n",
    "    f1.append((round(f1_score(y_train,y_preds),2))*100)\n",
    "    conf_mat.append(confusion_matrix(y_train,y_preds))\n",
    "\n",
    "# creating a dataframe with the metrics from the list and each algorithm name\n",
    "results_df = pd.DataFrame({\"Recall Score\":recall,\n",
    "                           \"Accuracy Score\":accuracy,\n",
    "                           \"Precision Score\":precision,\n",
    "                           \"f1 Score\":f1,\n",
    "                           \"Confusion Matrix\":conf_mat,\n",
    "                           \"Algorithm\":['eclf1_hard', 'eclf2_soft', 'eclf3_uniform']})\n",
    "\n",
    "# sorting algorithm name alphabetically and setting index to the algorithm name \n",
    "results_df.sort_values(by = 'Algorithm').set_index('Algorithm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c44521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def voting_predictions(X_train, y_train, X_validate, y_validate):\n",
    "    '''\n",
    "    Actions: Gets dataframe with evaluation scores for VotingClassifier that uses SVC, GradientBoost, and LogisticRegression classifiers as voting parties\n",
    "    '''\n",
    "    # setting random state\n",
    "    random_state = 1017\n",
    "    \n",
    "     # intitializing different classifiers\n",
    "    clf1 = SVC(random_state=random_state, probability=True)\n",
    "    clf2 = GradientBoostingClassifier(random_state=random_state)\n",
    "    clf3 = LogisticRegression(random_state = random_state)\n",
    "    clf4 = LogisticRegression(C=.25, random_state = random_state)\n",
    "    clf5 = LogisticRegression(C=.5, random_state = random_state)\n",
    "\n",
    "    # initializing voting classifier with top three classifiers from above\n",
    "    eclf = VotingClassifier(estimators=[\n",
    "        ('svc', clf1),('gbc', clf2), ('lr', clf3), ('lr.5', clf4), ('lr.25', clf5)])\n",
    "\n",
    "    # fitting the model on the resampled train data\n",
    "    eclf.fit(X_train, y_train)\n",
    "\n",
    "    # assign predictions to variable\n",
    "    y_preds = eclf.predict(X_validate)\n",
    "\n",
    "    # initialize lists to hold metrics\n",
    "    accuracy,precision,recall,f1,conf_mat= [],[],[],[],[]\n",
    "\n",
    "    # appending the metrics to each repsective metric list\n",
    "    accuracy.append((round(accuracy_score(y_validate,y_preds),2))*100)\n",
    "    precision.append((round(precision_score(y_validate,y_preds),2))*100)\n",
    "    recall.append((round(recall_score(y_validate,y_preds),2))*100)\n",
    "    f1.append((round(f1_score(y_validate,y_preds),2))*100)\n",
    "    conf_mat.append(confusion_matrix(y_validate,y_preds))\n",
    "\n",
    "    # creating a dataframe with the metrics from the list and each algorithm name\n",
    "    results_df = pd.DataFrame({\"Recall Score\":recall,\n",
    "                               \"Accuracy Score\":accuracy,\n",
    "                               \"Precision Score\":precision,\n",
    "                               \"f1 Score\":f1,\n",
    "                               \"Confusion Matrix\":conf_mat,\n",
    "                               \"Algorithm\":'VotingClassifier_uniform'})\n",
    "\n",
    "    # sorting algorithm name alphabetically and setting index to the algorithm name \n",
    "    return results_df.sort_values(by = 'Algorithm').set_index('Algorithm').T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62666c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_predictions(X_resample, y_resample, X_validate, y_validate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
